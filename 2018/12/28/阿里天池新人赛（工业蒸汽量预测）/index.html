<!DOCTYPE HTML>
<html lang="">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="Mute的博客">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yangliuxiao.top">
    <!--SEO-->

    <meta name="keywords" content="代码,机器学习,Python">


    <meta name="description" content="工业蒸汽量预测在上完数据挖掘课程之后，虽然对数据挖掘所涉及的基本算法知识有了初步的了解，但纸上得来终觉浅，绝知此事要躬行，所以在阿里天池大赛上找了这么个新人赛（除了新人赛其他的比赛都无从下手了）...">



<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">

    <!--Title-->


<title>阿里天池新人赛（工业蒸汽量预测） | Mute的博客</title>


    <link rel="alternate" href="/atom.xml" title="Mute的博客" type="application/atom+xml">


    <link rel="icon" href="/favicon.jpg">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>






    

</head>

</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header" style="background-image:url(/./img/background01.png)">
    <div class="main-header-box">
        <a class="header-avatar" href="/" title="Mute">
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                 <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://yangliuxiao.top">Mute的博客</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>Home</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>时间轴</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/about/"><i class="fa "></i>关于</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="阿里天池新人赛（工业蒸汽量预测）">
            
	            阿里天池新人赛（工业蒸汽量预测）
            
        </h1>
        <div class="post-meta">
    
        <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
            <a class="category-link" href="/categories/机器学习/">机器学习</a>
        </span>
    

    
        <span class="fa-wrap">
            <i class="fa fa-tags"></i>
            <span class="tags-meta">
                
                    <a class="tag-link" href="/tags/Python/">Python</a> <a class="tag-link" href="/tags/代码/">代码</a> <a class="tag-link" href="/tags/机器学习/">机器学习</a>
                
            </span>
        </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2018/12/28</span>
        </span>
        
    
</div>
            
            
    </div>
    
    <div class="post-body post-content">
        <h3 id="工业蒸汽量预测"><a href="#工业蒸汽量预测" class="headerlink" title="工业蒸汽量预测"></a>工业蒸汽量预测</h3><p>在上完数据挖掘课程之后，虽然对数据挖掘所涉及的基本算法知识有了初步的了解，但纸上得来终觉浅，绝知此事要躬行，所以在阿里天池大赛上找了这么个新人赛<del>（除了新人赛其他的比赛都无从下手了）</del>来练练手，一方面可以将老师教的知识巩固下，另一方面学习一下python机器学习库<strong>scikit-learn</strong>和数据处理库<strong>NumPy</strong>的使用。</p>
<p><a href="https://tianchi.aliyun.com/competition/entrance/231693/introduction" target="_blank" rel="noopener">阿里天池新人赛（工业蒸汽量预测）</a></p>
<blockquote>
<p> <strong>赛题背景</strong></p>
<p>火力发电的基本原理是：燃料在燃烧时加热水生成蒸汽，蒸汽压力推动汽轮机旋转，然后汽轮机带动发电机旋转，产生电能。在这一系列的能量转化中，影响发电效率的核心是锅炉的燃烧效率，即燃料燃烧加热水产生高温高压蒸汽。锅炉的燃烧效率的影响因素很多，包括锅炉的可调参数，如燃烧给量，一二次风，引风，返料风，给水水量；以及锅炉的工况，比如锅炉床温、床压，炉膛温度、压力，过热器的温度等。</p>
<p><strong>赛题描述</strong></p>
<p>经脱敏后的锅炉传感器采集的数据（采集频率是分钟级别），根据锅炉的工况，预测产生的蒸汽量。</p>
<p><strong>数据说明</strong>  </p>
<p>数据分成训练数据（train.txt）和测试数据（test.txt），其中字段”V0”-“V37”，这38个字段是作为特征变量，”target”作为目标变量。选手利用训练数据训练出模型，预测测试数据的目标变量，排名结果依据预测结果的MSE（mean square error）。</p>
</blockquote>
<p>以上就是赛题背景和赛题描述以及比赛给出的数据说明。赛题背景和赛题描述没什么好说的，只是说明了要做的到底是什么事情罢了，从中并没有什么可以对我们做数据分析有帮助的地方。而数据说明则告诉我们一共有38个特征变量，下载打开训练数据（train.txt）可以看到38个特征变量全都是连续数值类型，而且target也是连续数值类型的，这个时候我们第一反应就很自然地想到这应该是个<strong>回归问题</strong>。训练数据（train.txt）一共有2888条，测试数据（test.txt）一共有1925条。</p>
<p><img src="images\图片1.png" alt="images\图片1"></p>
<p>以上是这学期人工智能课程企业老师归纳的人工智能建模步骤。业务目标和数据获取这部分已经不用操心了，所以接下来要做的的就数据清洗和特征工程。</p>
<p>首次尝试思路：</p>
<p>读入数据之后，先计算每个特征与targtet之间的相关系数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#相关性分析</span></span><br><span class="line">column = data.columns.tolist()[:<span class="number">39</span>]</span><br><span class="line"></span><br><span class="line">mcorr = data[column].corr()</span><br><span class="line"></span><br><span class="line">mcorr_data = np.array(mcorr.target)</span><br><span class="line">print(<span class="string">'相关系数-------------------'</span>)</span><br><span class="line">print(mcorr_data)</span><br><span class="line">print(<span class="string">'--------------------'</span>)</span><br></pre></td></tr></table></figure>
<p>结果：</p>
<p><img src="images\图片3.png" alt="images\图片3"></p>
<p>从中可以看到有一些特征属性与target之间的相关系数很小，这些特征属性将会对回归结果有影响，所以去掉相关系数相对较小的一些特征属性。</p>
<p>然后将数据进行标准化之后，进行PCA降维。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">data_scaled = preprocessing.scale(data)</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">pca_data = pd.DataFrame(pca.fit_transform(data))</span><br></pre></td></tr></table></figure>
<p>使用sklearn封装的PCA方法，PCA方法参数n_components，如果设置为整数，则n_components=k。如果将其设置为小数，则说明降维后的数据能保留的信息。</p>
<p>PCA降维的维度选择很麻烦，但有sklearn，我们只需要将n_components设置为0.9即可，即表明降维后的数据能保留90%的信息，不需要选择具体维度。</p>
<p>对数据的处理过程就先到这里，然后开始选择机器学习算法。前文已经说过，从给出的数据来看，这应该是个回归问题，所以选择① 线性回归 ②梯度提升回归（GradientBoostingRegressor） ③ xgboost 来做回归预测。</p>
<p><strong>① 线性回归</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练数据和测试数据划分</span></span><br><span class="line"><span class="comment">#train_test_split(train_data,train_target,test_size=0.3, random_state=0)</span></span><br><span class="line"><span class="comment">#train_data：被划分的样本特征集 </span></span><br><span class="line"><span class="comment">#train_target：被划分的样本标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#test_size：如果是浮点数，在0-1之间，表示样本占比；如果是整数的话就是样本的数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#random_state：是随机数的种子。</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(pca_data, target, test_size=<span class="number">0.2</span>,random_state = <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line">linreg = LinearRegression()</span><br><span class="line">linreg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"截距和斜率*************"</span>)</span><br><span class="line">print(linreg.intercept_)</span><br><span class="line">print(linreg.coef_)</span><br><span class="line">print(<span class="string">"***********************"</span>)</span><br><span class="line"></span><br><span class="line">target_pred = linreg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用scikit-learn计算MSE</span></span><br><span class="line">print(<span class="string">"线性回归："</span>)</span><br><span class="line">print(metrics.mean_squared_error(y_test, target_pred))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">线性回归：</span><br><span class="line"><span class="number">0.12046985211850315</span></span><br></pre></td></tr></table></figure>
<p><strong>② 梯度提升回归（GradientBoostingRegressor）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">myGBR = GradientBoostingRegressor(alpha=<span class="number">0.9</span>, criterion=<span class="string">'friedman_mse'</span>, init=<span class="keyword">None</span>,</span><br><span class="line">                                  learning_rate=<span class="number">0.03</span>, loss=<span class="string">'huber'</span>, max_depth=<span class="number">15</span>,</span><br><span class="line">                                  max_features=<span class="string">'sqrt'</span>, max_leaf_nodes=<span class="keyword">None</span>,</span><br><span class="line">                                  min_impurity_decrease=<span class="number">0.0</span>, min_impurity_split=<span class="keyword">None</span>,</span><br><span class="line">                                  min_samples_leaf=<span class="number">10</span>, min_samples_split=<span class="number">40</span>,</span><br><span class="line">                                  min_weight_fraction_leaf=<span class="number">0.0</span>, n_estimators=<span class="number">300</span>,</span><br><span class="line">                                  presort=<span class="string">'auto'</span>, random_state=<span class="number">10</span>, subsample=<span class="number">0.8</span>, verbose=<span class="number">0</span>,</span><br><span class="line">                                  warm_start=<span class="keyword">False</span>)</span><br><span class="line">myGBR.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">target_pred_2 = myGBR.predict(X_test)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"GBR:"</span>)</span><br><span class="line">print(metrics.mean_squared_error(y_test, target_pred_2))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GBR:</span><br><span class="line"><span class="number">0.1169921728668063</span></span><br></pre></td></tr></table></figure>
<p><strong>③ xgboost</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model_xgb = xgb.XGBRegressor(booster=<span class="string">'gbtree'</span>,colsample_bytree=<span class="number">0.8</span>, gamma=<span class="number">0.1</span>, </span><br><span class="line">                             learning_rate=<span class="number">0.02</span>, max_depth=<span class="number">5</span>, </span><br><span class="line">                             n_estimators=<span class="number">500</span>,min_child_weight=<span class="number">0.8</span>,</span><br><span class="line">                             reg_alpha=<span class="number">0</span>, reg_lambda=<span class="number">1</span>,</span><br><span class="line">                             subsample=<span class="number">0.8</span>, silent=<span class="number">1</span>,</span><br><span class="line">                             random_state =<span class="number">42</span>, nthread = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model_xgb.fit(X_train, Y_train)</span><br><span class="line"></span><br><span class="line">y_pred = model_xgb.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">"XGBoost："</span>)</span><br><span class="line"></span><br><span class="line">print(metrics.mean_squared_error(y_test, y_pred))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">XGBoost：</span><br><span class="line"><span class="number">0.12128023196468511</span></span><br></pre></td></tr></table></figure>
<p>选取其中较好的结果（GBR），提交答案后发现结果并不如预期。</p>
<p><img src="images\图片4.png" alt="images\图片4"></p>
<p>出现这样的结果，很显然是过拟合的原因。所以参考了论坛中<a href="https://tianchi.aliyun.com/course/courseConsole?courseId=325&amp;chapterIndex=1&amp;sectionIndex=5" target="_blank" rel="noopener">新人赛工业蒸汽预测代码notebook分享-陆玮超</a>的思路之后，重新进行特征工程。</p>
<p>首先读取数据，将train数据的target字段去除后，与test数据合并，方便之后数据清洗和特征工程。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">train=pd.read_csv(<span class="string">'E:\\projects\\steamPrediction\\dataSet\\zhengqi_train.txt'</span>,sep=<span class="string">'\t'</span>)</span><br><span class="line">test=pd.read_csv(<span class="string">'E:\\projects\\steamPrediction\\dataSet\\zhengqi_test.txt'</span>,sep=<span class="string">'\t'</span>)</span><br><span class="line">train_x=train.drop([<span class="string">'target'</span>],axis=<span class="number">1</span>)<span class="comment"># 将train数据的target字段去除，用以训练模型</span></span><br><span class="line"></span><br><span class="line">all_data = pd.concat([train_x,test]) <span class="comment"># 将train与test数据合并</span></span><br></pre></td></tr></table></figure>
<p>而后需要观察一下训练集和测试集中的数据分布情况，将<strong>训练集和测试集中数据分布有很大差别的那些特征去除掉，从而减小误差。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据观察（可视化）</span></span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> all_data.columns:</span><br><span class="line">    seaborn.distplot(train[col])</span><br><span class="line">    seaborn.distplot(test[col])</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="images\图片2.png" alt="images\图片2"></p>
<p>以上是截取的V1特征变量和V5特征变量的可视化结果，可以明显地看出，V1变量测试集和训练集的数据分布基本类似，而V5变量在两个数据集中的分布则存在较大的差别，所以应该手动将V5特征变量剔除。</p>
<p>接下来进行<strong>数据标准化和偏态修正</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">all_data.drop([<span class="string">'V5'</span>,<span class="string">'V17'</span>,<span class="string">'V28'</span>,<span class="string">'V22'</span>,<span class="string">'V11'</span>,<span class="string">'V9'</span>,<span class="string">'V32'</span>,<span class="string">'V33'</span>,<span class="string">'V34'</span>],axis=<span class="number">1</span>,inplace=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据标准化</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">min_max_scaler = preprocessing.MinMaxScaler()</span><br><span class="line">data_minmax = pd.DataFrame(min_max_scaler.fit_transform(all_data),columns=all_data.columns)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment">#偏态修正</span></span><br><span class="line"></span><br><span class="line">data_minmax[<span class="string">'V0'</span>] = data_minmax[<span class="string">'V0'</span>].apply(<span class="keyword">lambda</span> x:math.exp(x))</span><br><span class="line">data_minmax[<span class="string">'V1'</span>] = data_minmax[<span class="string">'V1'</span>].apply(<span class="keyword">lambda</span> x:math.exp(x))</span><br><span class="line">data_minmax[<span class="string">'V6'</span>] = data_minmax[<span class="string">'V6'</span>].apply(<span class="keyword">lambda</span> x:math.exp(x))</span><br><span class="line">data_minmax[<span class="string">'V30'</span>] = np.log1p(data_minmax[<span class="string">'V30'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_scaled = pd.DataFrame(preprocessing.scale(data_minmax),columns = data_minmax.columns)</span><br><span class="line">train_x = X_scaled.loc[<span class="number">0</span>:len(train)<span class="number">-1</span>]</span><br><span class="line">test = X_scaled.loc[len(train):]</span><br><span class="line"></span><br><span class="line">Y=train[<span class="string">'target'</span>]</span><br></pre></td></tr></table></figure>
<p>特征选择部分，除了上文的可视化阶段，对测试集和训练集分布不同的特征去除之外，这里再根据<strong>方差和单变量进行特征选择</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#特征选择</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> f_regression</span><br><span class="line"></span><br><span class="line"><span class="comment">#方差</span></span><br><span class="line">threshold = <span class="number">0.85</span>                  </span><br><span class="line">vt = VarianceThreshold().fit(train_x)</span><br><span class="line"><span class="comment"># Find feature names</span></span><br><span class="line">feat_var_threshold = train_x.columns[vt.variances_ &gt; threshold * (<span class="number">1</span>-threshold)]</span><br><span class="line">train_x = train_x[feat_var_threshold]</span><br><span class="line">test = test[feat_var_threshold]</span><br><span class="line"></span><br><span class="line"><span class="comment">#单变量</span></span><br><span class="line">X_scored = SelectKBest(score_func=f_regression, k=<span class="string">'all'</span>).fit(train_x, Y)</span><br><span class="line">feature_scoring = pd.DataFrame(&#123;</span><br><span class="line">        <span class="string">'feature'</span>: train_x.columns,</span><br><span class="line">        <span class="string">'score'</span>: X_scored.scores_</span><br><span class="line">    &#125;)</span><br><span class="line">head_feature_num = <span class="number">18</span></span><br><span class="line">feat_scored_headnum = feature_scoring.sort_values(<span class="string">'score'</span>, ascending=<span class="keyword">False</span>).head(head_feature_num)[<span class="string">'feature'</span>]</span><br><span class="line">train_x_head = train_x[train_x.columns[train_x.columns.isin(feat_scored_headnum)]]</span><br><span class="line">X_scaled = pd.DataFrame(preprocessing.scale(train_x),columns = train_x.columns)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">test = test[test.columns[test.columns.isin(feat_scored_headnum)]]</span><br><span class="line">Y_scaled = pd.DataFrame(preprocessing.scale(test),columns = test.columns)</span><br></pre></td></tr></table></figure>
<p>接下来进行模型选择，这里选择了① SVR ② 线性回归  ③Lasso ④ ElasticNet ⑤ 多项式的KernelRidge ⑥ 线性的KernelRidge以及⑦ xgboost</p>
<p>最后对SVR、线性的KernelRidge、xgboost三种模型进行了简单的融合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型尝试</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor,  GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.kernel_ridge <span class="keyword">import</span> KernelRidge</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> RobustScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin, RegressorMixin, clone</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行10折的交叉验证</span></span><br><span class="line">n_folds = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsle_cv</span><span class="params">(model,train_x_head=train_x_head)</span>:</span></span><br><span class="line">    kf = KFold(n_folds, shuffle=<span class="keyword">True</span>, random_state=<span class="number">42</span>).get_n_splits(train_x_head)</span><br><span class="line">    rmse= -cross_val_score(model, train_x_head, Y, scoring=<span class="string">"neg_mean_squared_error"</span>, cv = kf)</span><br><span class="line">    <span class="keyword">return</span>(rmse)</span><br><span class="line"></span><br><span class="line">svr = make_pipeline( SVR(kernel=<span class="string">'linear'</span>))</span><br><span class="line"></span><br><span class="line">line = make_pipeline( LinearRegression())</span><br><span class="line">lasso = make_pipeline( Lasso(alpha =<span class="number">0.0005</span>, random_state=<span class="number">1</span>))</span><br><span class="line">ENet = make_pipeline( ElasticNet(alpha=<span class="number">0.0005</span>, l1_ratio=<span class="number">.9</span>, random_state=<span class="number">3</span>))</span><br><span class="line">KRR1 = KernelRidge(alpha=<span class="number">0.6</span>, kernel=<span class="string">'polynomial'</span>, degree=<span class="number">2</span>, coef0=<span class="number">2.5</span>)</span><br><span class="line">KRR2 = KernelRidge(alpha=<span class="number">1.5</span>, kernel=<span class="string">'linear'</span>, degree=<span class="number">2</span>, coef0=<span class="number">2.5</span>)</span><br><span class="line"></span><br><span class="line">model_xgb = xgb.XGBRegressor(booster=<span class="string">'gbtree'</span>,colsample_bytree=<span class="number">0.8</span>, gamma=<span class="number">0.1</span>, </span><br><span class="line">                             learning_rate=<span class="number">0.02</span>, max_depth=<span class="number">5</span>, </span><br><span class="line">                             n_estimators=<span class="number">500</span>,min_child_weight=<span class="number">0.8</span>,</span><br><span class="line">                             reg_alpha=<span class="number">0</span>, reg_lambda=<span class="number">1</span>,</span><br><span class="line">                             subsample=<span class="number">0.8</span>, silent=<span class="number">1</span>,</span><br><span class="line">                             random_state =<span class="number">42</span>, nthread = <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = rmsle_cv(svr)</span><br><span class="line">print(<span class="string">"\nSVR 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line">svr.fit(train_x_head,Y)</span><br><span class="line">score = rmsle_cv(line)</span><br><span class="line">print(<span class="string">"\nLine 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line">score = rmsle_cv(lasso)</span><br><span class="line">print(<span class="string">"\nLasso 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line">score = rmsle_cv(ENet)</span><br><span class="line">print(<span class="string">"ElasticNet 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line"></span><br><span class="line">score = rmsle_cv(KRR2)</span><br><span class="line">print(<span class="string">"Kernel Ridge2 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line">KRR2.fit(train_x_head,Y)</span><br><span class="line"></span><br><span class="line">head_feature_num = <span class="number">18</span></span><br><span class="line">feat_scored_headnum = feature_scoring.sort_values(<span class="string">'score'</span>, ascending=<span class="keyword">False</span>).head(head_feature_num)[<span class="string">'feature'</span>]</span><br><span class="line">train_x_head2 = train_x[train_x.columns[train_x.columns.isin(feat_scored_headnum)]]</span><br><span class="line">X_scaled = pd.DataFrame(preprocessing.scale(train_x),columns = train_x.columns)</span><br><span class="line">score = rmsle_cv(KRR1,train_x_head2)</span><br><span class="line">print(<span class="string">"Kernel Ridge1 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line"></span><br><span class="line">head_feature_num = <span class="number">22</span></span><br><span class="line">feat_scored_headnum = feature_scoring.sort_values(<span class="string">'score'</span>, ascending=<span class="keyword">False</span>).head(head_feature_num)[<span class="string">'feature'</span>]</span><br><span class="line">train_x_head3 = train_x[train_x.columns[train_x.columns.isin(feat_scored_headnum)]]</span><br><span class="line">X_scaled = pd.DataFrame(preprocessing.scale(train_x),columns = train_x.columns)</span><br><span class="line">score = rmsle_cv(model_xgb,train_x_head3)</span><br><span class="line">print(<span class="string">"Xgboost 得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br><span class="line">model_xgb.fit(train_x_head,Y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#简单模型融合</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AveragingModels</span><span class="params">(BaseEstimator, RegressorMixin, TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, models)</span>:</span></span><br><span class="line">        self.models = models</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历所有模型，拟合数据</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">        self.models_ = [clone(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.models]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> model <span class="keyword">in</span> self.models_:</span><br><span class="line">            model.fit(X, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预估，并对预估结果值做average</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        predictions = np.column_stack([</span><br><span class="line">            model.predict(X) <span class="keyword">for</span> model <span class="keyword">in</span> self.models_</span><br><span class="line">        ])</span><br><span class="line">        <span class="keyword">return</span> np.mean(predictions, axis=<span class="number">1</span>)   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">averaged_models = AveragingModels(models = (svr,KRR2,model_xgb))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">score = rmsle_cv(averaged_models)</span><br><span class="line"></span><br><span class="line">print(<span class="string">" 对基模型集成后的得分: &#123;:.4f&#125; (&#123;:.4f&#125;)\n"</span>.format(score.mean(), score.std()))</span><br></pre></td></tr></table></figure>
<p>最后得到结果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">SVR 得分: <span class="number">0.1276</span> (<span class="number">0.0555</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Line 得分: <span class="number">0.1290</span> (<span class="number">0.0544</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Lasso 得分: <span class="number">0.1290</span> (<span class="number">0.0543</span>)</span><br><span class="line"></span><br><span class="line">ElasticNet 得分: <span class="number">0.1290</span> (<span class="number">0.0543</span>)</span><br><span class="line"></span><br><span class="line">Kernel Ridge2 得分: <span class="number">0.1269</span> (<span class="number">0.0531</span>)</span><br><span class="line"></span><br><span class="line">Kernel Ridge1 得分: <span class="number">0.1306</span> (<span class="number">0.0565</span>)</span><br><span class="line"></span><br><span class="line">Xgboost 得分: <span class="number">0.1312</span> (<span class="number">0.0573</span>)</span><br><span class="line"></span><br><span class="line">对基模型集成后的得分: <span class="number">0.1225</span> (<span class="number">0.0541</span>)</span><br></pre></td></tr></table></figure>

    </div>
    
    <div class="post-footer">
        <div>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2019/03/05/面试问题（一）/" class="pre-post btn btn-default" title="面试问题（一）">
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">面试问题（一）</span>
        </a>
    
    
        <a href="/2018/12/05/无监督对偶学习在图像转换领域的应用/" class="next-post btn btn-default" title="无监督对偶学习在图像转换领域的应用">
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">无监督对偶学习在图像转换领域的应用</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>






                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">Table of Contents</h3>
        
            <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#工业蒸汽量预测"><span class="toc-text">工业蒸汽量预测</span></a></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
        Total:
        <strong id="busuanzi_value_site_pv">
            <i class="fa fa-spinner fa-spin"></i>
        </strong>
        &nbsp; | &nbsp;
        Visitors:
        <strong id="busuanzi_value_site_uv">
            <i class="fa fa-spinner fa-spin"></i>
        </strong>
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2017
                </span>

            </div>
        </div>
    </div>
</div>






    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<script src="/js/app.js?rev=@@hash"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"log":false});</script></body>
</html>